<!doctype html>
<html lang="en">
<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="icon" href="img/favicon.png" type="image/png">
    <title>Claynce - SDP Group #2</title>
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="css/bootstrap.css">
    <link rel="stylesheet" href="css/themify-icons.css">
    <link rel="stylesheet" href="vendors/fontawesome/css/all.min.css">
    <link rel="stylesheet" href="vendors/owl-carousel/owl.carousel.min.css">
    <link rel="stylesheet" href="vendors/animate-css/animate.css">
    <!-- main css -->
    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/responsive.css">

    <style>
      .centeredBlock {
        display: block;
        margin-left: auto;
        margin-right: auto;
      }
    </style>

    <!-- Customized CSS -->
    <link rel="stylesheet" href="css/sdp-customized/customized.css">
</head>
<body>

    <!--================Header Menu Area =================-->
    <header class="header_area">
        <div class="main_menu">
            <nav class="navbar navbar-expand-lg navbar-light">
                <div class="container">
                    <!-- Brand and toggle get grouped for better mobile display -->
                    <h1 id="claynce-header-text">Claynce</h1>
                    <button class="navbar-toggler" type="button" data-toggle="collapse"
                    data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                    aria-expanded="false" aria-label="Toggle navigation">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <!-- Collect the nav links, forms, and other content for toggling -->
                    <div class="collapse navbar-collapse offset" id="navbarSupportedContent">
                        <ul class="nav navbar-nav menu_nav ml-auto">
                            <li class="nav-item"><a class="nav-link" href="index.html">WELCOME</a></li>
                            <li class="nav-item"><a class="nav-link" href="system.html">SYSTEM</a></li>
                            <li class="nav-item submenu dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" role="button"
                                aria-haspopup="true" aria-expanded="false">HOW IT WORKS</a>
                                <ul class="dropdown-menu">
                                    <li class="nav-item"><a class="nav-link" href="overview.html">Overview</a></li>
                                    <li class="nav-item"><a class="nav-link" href="frontend.html">Front-End</a></li>
                                    <li class="nav-item"><a class="nav-link" href="backend.html">Back-End</a></li>
                                    <li class="nav-item"><a class="nav-link" href="robot.html">Robot</a></li>
                                </ul>
                            </li>
                            <li class="nav-item"><a class="nav-link" href="evaluation.html">EVALUATION</a></li>
                            <li class="nav-item"><a class="nav-link" href="budget.html">BUDGET</a></li>
                            <li class="nav-item"><a class="nav-link" href="team.html">TEAM</a></li>
                        </ul>
                    </div>
                </div>
            </nav>
        </div>
    </header>
    <!--================Header Menu Area =================-->


    <!--================Hero Banner Area Start =================-->
    <section id="banner" class="hero-banner d-flex align-items-center">
        <div class="container text-center">
            <h2>Robot</h2>
        </div>
    </section>
    <!--================Hero Banner Area End =================-->

    <!--================Introduction Start =================-->
    <section class="sample-text-area">
        <div class="container">
            <div id="corps_central">
                <section>
                  <h1>Robot Localization Techniques</h1>
                  <p>
                    A tennis court is a challenging environment from a robot localization perspective. Several features contribute to such a state of things:
                  </p>
                  <ul style="list-style-type: circle;">
                    <li>Big area</li>
                    <li>Symmetric layout</li>
                    <li>Few characteristic keypoints</li>
                  </ul>
                  <p>
                    For that reasons, many common localization techniques such as SLAM with LiDAR perform rather poorly (according to our SDP Navigation Expert). Therefore, we decided to employ a more innovative visual localization technique using a set of ArUco markers. It requires the court to be altered a bit by placing the markers around the court, but it is not intrusive as the markers can be placed outside the court itself, not interrupting the flow of the game. It does not come without any pitfalls, but supported by the fine-tuned odometry it provides a satisfactory result, that can reliably orient our system on the tennis court.
                  </p>
                  <figure>
                    <img src="./resources/imgs/aruco.png" class="centeredBlock" width="100" height="100"/>
                    <figcaption style="text-align: center;">Sample ArUco marker (id: 23)</figcaption>
                  </figure>

                  <p>
                    The next sections provide a more in-depth explanation of the theoretical background and our implementation of those systems.
                  </p>


                  <h3>ArUco Visual Localization</h3>

                  <p>
                    The internal map of the environment consists of locations and poses of ArUco markers in the tennis court environment. Currently, the translation and orientation of markers, with respect to the origin of the "world" coordinate system (centre of the court), is determined during the system setup phase. The user places markers at known positions around the court and contacts our team specifying the placement of the markers so we can send them Claynce with the appropriate court map preinstalled.
                  </p>
                  <p>
                    In order not to over constrain the user. We do not require the exact positioning of the markers. Instead, we prepared a set of guidelines regarding the layout of ArUco markers. Following these recommendations ensures that the system behaves as efficiently as possible:
                  </p>
                  <ul style="list-style-type: circle;">
                    <li>Markers should be placed along the court, on both sides - there is no need to have markers on the ends of the court</li>
                    <li>Markers should be placed vertically</li>
                    <li>Markers should be placed 60 centimetres  above the ground - so they don't get covered with dusty clay that easily</li>
                    <li>Markers should not be covered by any objects</li>
                    <li>Markers shouldn't be placed further than 9 meters away from the court’s centerline - so the robot can detect them more or less in the middle of the tennis court and correct the pose estimation error accumulated when navigating using odometry</li>
                    <li>Markers should be 2 meters away from each other - this ensures that they are distributed uniformly + robot can see 2-3 markers at most times</li>
                  </ul>
                  <img src="./resources/imgs/aruco_layout.png" class="centeredBlock" />

                  <p>
                    This is a temporary solution implemented in the prototype of Claynce that enabled us to create a proof of concept.
                    In future development we would like to:
                  </p>
                  <ol>
                    <li>Create a configuration panel in our app, so the user can dynamically specify/update the positions of the markers associated with the robot.</li>
                    <li>Implement automatic <a href="http://wiki.ros.org/aruco_mapping">ArUco mapping</a> that could enable the robot itself to generate the court map.</li>
                    <li>Add a many-to-many relationship between court maps and robots, so one robot could be used on multiple courts without the need for reconfiguration.</li>
                  </ol>
                  <p>
                    Going back to what we currently have, the translation and rotation of each marker are hardcoded in the robot's software.
                    Each ArUco marker has a unique ID with which its positions is associated and published as a static transform from the world coordinate frame to the coordinate system of the ArUco marker.
                  </p>

                  <p>
                    To make the pose estimation as accurate as possible, our robot needs to see some markers.
                    For that, we use 3 Full HD Raspberry Pi cameras. Higher-resolution helps us to more accurately determine the position of the robot with respect to the marker.
                    We are willing to sacrifice the higher ratecenteredImg of pose estimations due to higher resolution, as the frequency is not as crucial when we have 3 cameras working simultaneously.
                  </p>
                  <p>
                    In our setup, two cameras are facing forward and are rotated by 30 degrees to the left and right.
                    Such configuration allows us to reliably detect markers when moving towards markers and/or in parallel to them.
                    One camera at the back is responsible for detecting markers when moving away from the markers.
                  </p>
                  <p>
                    Because it turns out that detecting markers that are at the angle to the camera gives better results. The range of marker detection of front cameras is set to 9 meters, whilst the rear one is limited to 5 meters only. The values have been set based on the results of our tests, which are presented on thee <a href="evaluation.html">Evaluation page</a>.
                    This way, the position estimation error should not exceed 50 centimetres.
                  </p>

                  <p>
                    For the visual localisation to work, camera information and raw images are published by the device manager from ROSification library provided by Webots.
                    The Image messages are published over a topic.
                    A specialized node converts the messages to OpenCV frame using CvBridge tool.
                    That frame is further processed (e.g. converting to greyscale), and ArUcos are detected.
                    The process of marker recognition and camera -> marker transform computation is covered by OpenCV library.
                    In the end, the frame is annotated with IDs of the markers and their bounding boxes.
                    The modified frame is converted back to the Image message and published over a different topic for debugging purposes.
                  </p>

                  <figure>
                    <img src="./resources/imgs/detected_markers.png" class="centeredBlock"/>
                    <figcaption style="text-align: center;">Example of marker detection. Four markers have been detected, but only three leftmost are within the range for pose estimation.</figcaption>
                  </figure>


                  <p>
                    A brief overview of the implemented algorithm:
                  </p>
                  <ol>
                    <li>Initialise an empty list of pose estimates.</li>
                    <li>Get transform from the camera to the robot's base link coordinate system.</li>
                    <li>Detect markers in the frame.</li>
                    <li>Filter out the ArUcos that are too far for reliable post estimation.</li>
                    <li>For each ArUco in a list of ArUcos:
                    <ol type="a">
                      <li>Get transform from the world to a marker's coordinate system</li>
                      <li>rvec, tvec = estimatePose(aruco)<br />(tvec is a translation of the transform from camera to marker, rvec is a rotation of the transform)</li>
                      <li>Invert rvec and tvec transforms to get a transform from the ArUco to the camera coordinate system</li>
                      <li>Convert all the transforms to homogeneous transformation matrix (HTM) form</li>
                      <li>Compose all HTMs in order: world -> ArUco -> camera -> robot's base link</li>
                      <li>Extract the x, y from the translation part and yaw from the rotation part of the final HTM.</li>
                    </ol>
                    </li>
                    <li>Return the averaged x, y, yaw over all estimates as the final estimate.</li>
                  </ol>
                  <p>
                    Below you can see the visual localization in action. The green arrow in the video represents the actual position of the robot. The red one is the position estimate. The axes on the top represent the positions of the markers. One can see that at the beginning the estimate is far from the actual position (estimates diverge from the actual position when the robot uses only the odometry). The moment when the camera detects markers on the wall, the pose is updated with a much more accurate estimate.
                  </p>
                  <video class="centeredBlock" width="420" height="480" controls style="margin-bottom: 40px;">
                    <source src="./resources/videos/visual_nav_sample.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </section>
                <section>
                  <h1>Odometry</h1>
                  <p>
                    Because of the way the cameras are placed and configured, there is a "blind zone" on the court in which the robot cannot determine its position from the ArUco markers. This happens because markers on both sides of the court are too far from the robot.
                  </p>
                  <figure>
                    <img src="./resources/imgs/blind_zone.png" class="centeredBlock" height="300px" />
                    <figcaption style="text-align: center;">Blind zone when moving to the north. Adapted from: <a href="https://www.promain.co.uk/media/catalog/product/cache/8fa5dff5584a74f50bf3124aec91b902/2/0/20-d-45-brick-red.gif" >source</a></figcaption>
                  </figure>
                  <p>
                    Since the robot enters the blind zone after every turn away from the markers, it is important that the robot still can assess its position. For this reason, we implemented odometry as a fallback localization procedure. It is a "fallback" method in the sense that visual localization always takes precedence over odometry. Its purpose is to get the robot through the blind zone until it can again reliably detect the markers on the other side of the court.
                  </p>
                  <p>
                    Odometry has been implemented based on the <a href="https://www.hmc.edu/lair/ARW/ARW-Lecture01-Odometry.pdf">"Odometry Kinematics"</a> slides from Harvey Mudd College. The main parameters: wheel radius and wheel gap, have been tuned using an experimental approach. Their final values vary significantly from "theoretical" ones. But they ensure that wheel encoder readings give a reliable estimation for the forward motion and 90-degree turns. The tasks that are the most crucial for entering and leaving the blind zone. Below you can see that odometry does well when moving straight and performing ~90-degree turns, but works much worse with other turns.
                  </p>
                  <video class="centeredBlock" width="420" height="480" controls style="margin-bottom: 40px;">
                    <source src="./resources/videos/odom_sample.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </section>
                <section>
                  <h1>Pattern Path Generation</h1>
                  <p>
                    Currently, Claynce supports 2 patterns that are widely used by the tennis community: zig-zag and circular. A path that the robot has to follow in a selected sweeping program is generated recursively. This makes it highly extensible to suit everyone's needs. The algorithm starts at the net and finishes at the end of the court. The path is generated for one half only and then the points for the other half are generated by reflecting the existing path in the net. This ensures two things. Firstly, that the robot starts sweeping the second half on the same side that it finished cleaning the first one. Secondly, that the robot finishes the program on the opposite half and the same side of the court on which it started the pattern. Finally, the robot gets an instruction to come back to the net so the user can pick it up.
                  </p>
                  <p>
                    Our algorithms are highly parametrised and very flexible to suit every user needs. Currently, we have fixed parameters for pattern generation, but in the future, we would like to expose those parameters via an app so users can create patterns that are adapted to their requirements. Regardless of the pattern, module generating paths takes into account:
                  </p>
                  <ul style="list-style-type: circle;">
                    <li>width of the court</li>
                    <li>length of the court</li>
                    <li>width of the net</li>
                    <li>desired net overlap when the robot traverses the court</li>
                  </ul>
                  <figure class="centeredBlock">
                    <img src="./resources/imgs/zig-zag.png" width="49%" />
                    <img src="./resources/imgs/circular.png" width="49%" />
                    <figcaption style="text-align: center;">Currently supported cleaning patterns.</figcaption>
                  </figure>
                </section>
                <section>
                  <h1>Navigation</h1>
                  <p> The navigation algorithm is simple: the robot is given a list of (x,y) points in the court’s reference system - the points are determined from the position of the ArUco markers.
                  Claynce knows its position on the court, and hence is able to travel to the first point on the list. Once it’s in range of that point, the list will be updated to remove it, and the robot will look at the next point to check by how much it needs to turn to reach it.
                  This approach allows Claynce to navigate to literally any point from the list, as long as it knows its position correctly. This makes the algorithm very versatile, although right now we’re not using it to full potential - the web application allows you to choose one of two predefined patterns (either a zigzag one, or a circular one).
      <p>While the robot is moving to the next point, it is possible for it to go off-track due to friction or sliding of the wheels or maybe something else. To remedy this, there exists an angle correction algorithm: given the location of the next point and the position of Claynce on the court, it’s possible to calculate the distance between them, as well as the angle of the direction of movement (with respect to the court’s reference frame). If this calculated angle is not the same as the angle at which the robot is travelling now, Claynce will adjust its wheel velocities to turn more to the side to face the destination straight on.
      For example, see this image:
      <figure>
                    <img src="./resources/imgs/angle-correction.jpg" class="centeredBlock" height="300px" />
                    <figcaption style="text-align: center;">When Claynce performs angle correction</figcaption>
                  </figure>
      The robot has not turned properly, meaning that the destination is not straight ahead. The algorithm calculates the angle of travel (from the set horizontal axis x) alpha, as well as the angle by how much the course needs to be adjusted (beta).
      Using this information and basic trigonometry, the robot changes the velocities of the wheels so that it turns right until beta reaches almost zero - it will never exactly be zero, so some tolerance (0.1 degrees) is allowed.
      After that it starts going straight again, because the destination is now right in front of it. This algorithm runs constantly, so the robot continuously compares the angles and adjusts its velocity if necessary.
      </p>
                  </p>
                  <p>

      <b>Obstacle avoidance: </b>
      Claynce has three distance sensors on the front to allow it to detect obstacles. To retain a complete clay pattern, Claynce will not take any detours in the cleaning process, but instead will stop in front of any obstacles, notify its user, and wait until they are removed.
      The distance sensors are placed in such a way as to detect the obstacles that are 40 cm in front, or a bit to the sides of the robot (the ones at the sides may stop the net from moving forward); hence one sensor points straight, while the other two are point at 30-degree angles from the robot’s centerline.
      We opted for sensors with a shorter detection range to allow Claynce stop in front of reasonably small obstacles, while ignoring the net poles or the walls of the court, which are usually farther away from it.

                  </p>
                    <!--  <video class="centeredBlock" width="420" height="480" controls style="margin-bottom: 40px;">
                    <source src="resources/videos/obst.mp4" type="video/mp4"> -->
                </section>
              </div>        </div>
    </section>
    <!--================Concept End =================-->


    <!-- ================ Start footer Area ================= -->
    <footer class="footer-area">
        <div class="container">
        <div class="footer-bottom row align-items-center text-center text-lg-left no-gutters">
            <p class="footer-text m-0 col-lg-8 col-md-12"><!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
Copyright &copy;<script>document.write(new Date().getFullYear());</script> All rights reserved | This template is made with <i class="fa fa-heart" aria-hidden="true"></i> by <a href="https://colorlib.com" target="_blank">Colorlib</a>
<!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. --></p>
            <div class="col-lg-4 col-md-12 text-center text-lg-right footer-social">
                Creatively crafted by SDP Group #2
            </div>
        </div>
        </div>
    </footer>
<!-- ================ End footer Area ================= -->






<!-- Optional JavaScript -->
<!-- jQuery first, then Popper.js, then Bootstrap JS -->
<script src="js/jquery-2.2.4.min.js"></script>
<script src="js/popper.js"></script>
<script src="js/bootstrap.min.js"></script>
<script src="vendors/owl-carousel/owl.carousel.min.js"></script>
<script src="js/jquery.ajaxchimp.min.js"></script>
<script src="js/waypoints.min.js"></script>
<script src="js/mail-script.js"></script>
<script src="js/contact.js"></script>
<script src="js/jquery.form.js"></script>
<script src="js/jquery.validate.min.js"></script>
<script src="js/mail-script.js"></script>
<script src="js/theme.js"></script>
</body>
</html>
